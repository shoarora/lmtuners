tokenizer_type: wordpiece
model_type: electra

model:
  generator_name: ${model_type}_small_generator
  discriminator_name: ${model_type}_small_discriminator

  tokenizer_type: ${tokenizer_type}
  tokenizer_path: ./tokenizers/wikipedia-${tokenizer_type}

  training:
    total_steps: ${trainer.max_steps}

data:
  dataset_path: wikipedia
  dataset_version: 20200501.en
  batch_size: 128
  num_workers: 4
  column: text
  block_size: 128
  mlm_probability: 0.15
  pretokenize: true

trainer:
  gpus: 1
  fast_dev_run: false
  max_steps: 1e6
  limit_val_batches: 0.1
  val_check_interval: 0.025
  progress_bar_refresh_rate: 1000

logger:
  type: wandb
  args:
    name: ${model_type}-${tokenizer_type}-small-wikipedia
    project: transformers-trainers-v0.2.0
    entity: shoarora
    log_model: true
